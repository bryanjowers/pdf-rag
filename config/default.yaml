# Legal LLM RAG Pipeline Configuration
# Phase 3: Week 1 - Bbox + Entities + Embeddings
# Version: 2.3.0

metadata:
  config_version: "2.3.0"
  # config_hash will be auto-computed at runtime

# PDF Classification thresholds
classification:
  # Digital vs Scanned PDF routing
  percent_digital_cutoff: 0.75  # â‰¥75% text layer â†’ digital, else scanned

  # Confidence thresholds
  confidence_low_min: 0.65      # Below this = high confidence scanned
  confidence_low_max: 0.85      # Above this = high confidence digital

  # ðŸš¨ HARD LIMIT: NO PDF over this page count will be processed
  max_pages_absolute: 200       # Safety guardrail - prevents GPU exhaustion

  # Fallback thresholds
  max_pages_for_fallback: 1000  # Skip OlmOCR-2 fallback beyond this (quarantine instead)
  min_text_yield_per_page: 100  # If Docling produces <100 chars/page, trigger fallback

  # Parallelization
  parallel_workers: 8           # Number of parallel workers for batch classification

  # Image detection (detects pre-OCR'd scanned PDFs)
  image_detection:
    enabled: true                         # Enable full-page image detection
    coverage_threshold: 0.80              # 80%+ page coverage = full-page scan
    sample_threshold: 0.50                # 50%+ sampled pages with scans = classified as scanned
    sample_large_pdfs: true               # Use stratified sampling for >10 pages
    sample_size: 15                       # Number of pages to sample (5 strata Ã— 3 pages each)
    early_exit_scan_count: 3              # âš¡ Stop sampling after finding this many scans (early exit optimization)

# Text chunking parameters
chunking:
  token_target: 1400     # Target chunk size
  token_min: 800         # Minimum acceptable chunk size
  token_max: 2000        # Maximum acceptable chunk size

  # QA thresholds
  qa_warn_below: 700     # Warn if chunk has fewer tokens
  qa_warn_above: 2200    # Warn if chunk exceeds this
  qa_warn_threshold: 0.05  # Warn if >5% of chunks out of range
  qa_fail_threshold: 0.10  # Fail batch if >10% out of range

# XLSX/CSV processing
xlsx:
  # Sheet size thresholds
  rows_small_threshold: 5000    # Sheets â‰¤ this â†’ single chunk
  cols_small_threshold: 25      # Sheets â‰¤ this width â†’ single chunk

  # Chunking parameters for large sheets
  max_rows_per_chunk: 2000      # Hard cap on chunk size

  # Table boundary detection
  blank_row_threshold: 0.90     # â‰¥90% empty cells = blank row
  schema_change_threshold: 0.30 # >30% column difference = new table
  header_detection_threshold: 0.80  # â‰¥80% non-numeric = likely header

  # Processing options
  skip_hidden_sheets: true
  expand_merged_cells: true
  evaluate_formulas: true       # Compute formula values (don't preserve formula text)

# Table handling
tables:
  preserve_format: true          # Maintain Markdown table structure
  max_rows_per_chunk: 2000       # Split large tables at this row count
  include_footnotes: true        # Extract text below tables that looks like notes
  expand_merged_cells: true      # Repeat merged cell text across span

  # Footnote detection patterns (case-insensitive)
  footnote_indicators:
    - "^"      # Superscript numbers: ^1, ^2
    - "*"      # Asterisk markers
    - "â€ "      # Dagger symbols
    - "â€¡"      # Double dagger
    - "[1]"    # Bracketed numbers
    - "(a)"    # Parenthetical letters
    - "Note:"  # Explicit note markers

  # Validation
  check_column_consistency: true  # Verify all rows have same column count
  max_footnote_scan_lines: 10     # Look ahead this many lines after table

# Entity extraction settings
entity_extraction:
  enabled: true                   # Toggle entity extraction (requires OpenAI API key)
  openai_api_key: null            # If null, uses OPENAI_API_KEY environment variable
  extractor: "gpt-4o-mini"        # Model to use for extraction
  normalize: true                 # Normalize and deduplicate entities
  track_costs: true               # Track token usage and estimated costs

  # Entity types to extract (legal document focus)
  entity_types:
    - PERSON      # Individuals (grantors, grantees, witnesses, notaries)
    - PARCEL      # Property identifiers (tracts, legal descriptions)
    - DATE        # Recording/execution/effective dates
    - AMOUNT      # Monetary values (purchase prices, consideration)
    - ORG         # Organizations (corporations, LLCs, government entities)

  # Legal roles to identify
  legal_roles:
    - grantor     # Party conveying property/rights
    - grantee     # Party receiving property/rights
    - subject     # Subject matter of agreement
    - witness     # Witness to execution
    - notary      # Notary public

# Embedding generation (for RAG semantic search)
embeddings:
  enabled: true                     # Toggle embedding generation
  model: "all-mpnet-base-v2"        # sentence-transformers model name
  batch_size: 32                    # Batch size for generation
  normalize: true                   # Normalize embeddings to unit vectors
  dimension: 768                    # Embedding dimension (auto-detected from model)

# Qdrant vector database (for RAG)
qdrant:
  enabled: false                    # Toggle Qdrant upload
  host: "localhost"                 # Qdrant server host
  port: 6333                        # Qdrant server port
  collection_name: "legal_docs"     # Collection name
  in_memory: false                  # Use in-memory storage (testing only)
  batch_size: 100                   # Batch size for upload

# Processor settings
processors:
  # Parallel processing for digital PDFs (Docling + entities + embeddings)
  # Optimized with thread-local resource reuse (see handlers/pdf_digital.py)
  # Testing shows: 4 workers = 18.4s/file avg, 8 workers = ~10-11s/file (1.6-1.8x faster)
  # GPU-bound: No (L4 has 23GB, barely used). CPU-bound: Yes (Docling)
  digital_pdf_workers: 8          # Number of concurrent digital PDFs (increased from 4)

  # OlmOCR-2 settings (optimized for batch processing 100s of PDFs)
  olmocr:
    model_id: "allenai/olmOCR-2-7B-1025-FP8"
    target_image_dim: "1288"      # Keep quality for accuracy
    gpu_memory_utilization: 0.8   # 80% GPU memory (safe default)

    # File-level batching (NEW - 2-3x speedup!)
    enable_file_batching: true    # Enable batching multiple PDFs (amortizes model loading)
    default_batch_size: 10        # Number of PDFs per batch (10 files = ~58s saved)

    # Page-level settings
    default_workers: 12           # Page-level parallelism within documents
    pages_per_group: 10           # Page grouping for OlmOCR work queue

  # Docling settings (placeholder - adjust based on actual API)
  docling:
    api_timeout: 300              # 5 minute timeout
    enable_table_detection: true
    enable_section_detection: true
    output_format: "markdown"

  # Fallback settings
  retry_attempts: 2               # Retry count for transient errors
  retry_delay_seconds: 5          # Wait between retries

# Output schema
schema:
  version: "2.3.0"

  # Required JSONL fields
  required_fields:
    - id
    - doc_id
    - chunk_index
    - text
    - attrs
    - source
    - metadata

  # Metadata fields to include
  metadata_fields:
    - schema_version
    - file_type
    - mime_type
    - hash_input_sha256
    - processor
    - processor_version
    - processed_at
    - batch_id
    - processing_duration_ms
    - confidence_score
    - warnings

# Storage paths (relative to GCS mount base)
storage:
  gcs_mount_base: "/mnt/gcs/legal-ocr-results"

  # Input/output directories
  input_bucket: "/mnt/gcs/legal-ocr-pdf-input"
  rag_staging: "rag_staging"

  # Output subdirectories
  html_output: "rag_staging/html"
  markdown_output: "rag_staging/markdown"
  jsonl_output: "rag_staging/jsonl"
  log_dir: "logs"
  report_dir: "reports"
  manifest_dir: "manifests"
  inventory_dir: "inventory"
  quarantine_dir: "quarantine"

  # Lock file
  lock_file: ".process_documents.lock"

  # Atomic write settings
  temp_suffix: ".tmp"
  success_marker: "_SUCCESS"

# Watch mode settings
watch:
  default_interval_seconds: 60
  verify_mount_health: true

# Logging
logging:
  level: "INFO"
  format: "structured_csv"
  include_timestamps: true
