2025-10-29 19:58:45,059 - __main__ - INFO - If you run out of GPU memory during start-up or get 'KV cache is larger than available memory' errors, retry with lower values, e.g. --gpu_memory_utilization 0.80  --max_model_len 16384
INFO:olmocr.check:pdftoppm is installed and working.
2025-10-29 19:58:45,065 - __main__ - INFO - Got --pdfs argument, going to add to the work queue
2025-10-29 19:58:45,066 - __main__ - INFO - Loading file at /home/bryanjowers/pdf-rag/pdf_input/SDTO_170.0 ac 12-5-2022.pdf as PDF document
2025-10-29 19:58:45,066 - __main__ - INFO - Found 1 total pdf paths to add

Sampling PDFs to calculate optimal length:   0%|          | 0/1 [00:00<?, ?it/s]
Sampling PDFs to calculate optimal length: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 75.19it/s]
2025-10-29 19:58:45,080 - __main__ - INFO - Calculated items_per_group: 17 based on average pages per PDF: 28.00
2025-10-29 19:58:45,184 - __main__ - INFO - Starting pipeline with PID 49784
2025-10-29 19:58:45,184 - __main__ - INFO - Downloading model with hugging face 'allenai/olmOCR-2-7B-1025-FP8'

Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]
Fetching 19 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 16609.37it/s]
2025-10-29 19:58:45,368 - __main__ - INFO - Using internal server at http://localhost:30024/v1
INFO:olmocr.work_queue:Initialized queue with 1 work items
2025-10-29 19:58:45,475 - __main__ - WARNING - Attempt 1: Please wait for vllm server to become ready...
2025-10-29 19:58:46,529 - __main__ - WARNING - Attempt 2: Please wait for vllm server to become ready...
2025-10-29 19:58:47,567 - __main__ - WARNING - Attempt 3: Please wait for vllm server to become ready...
2025-10-29 19:58:48,603 - __main__ - WARNING - Attempt 4: Please wait for vllm server to become ready...
2025-10-29 19:58:48,954 - vllm - INFO - INFO 10-29 19:58:48 [__init__.py:216] Automatically detected platform cuda.
2025-10-29 19:58:49,641 - __main__ - WARNING - Attempt 5: Please wait for vllm server to become ready...
2025-10-29 19:58:50,684 - __main__ - WARNING - Attempt 6: Please wait for vllm server to become ready...
2025-10-29 19:58:51,755 - __main__ - WARNING - Attempt 7: Please wait for vllm server to become ready...
2025-10-29 19:58:52,795 - __main__ - WARNING - Attempt 8: Please wait for vllm server to become ready...
2025-10-29 19:58:53,834 - __main__ - WARNING - Attempt 9: Please wait for vllm server to become ready...
2025-10-29 19:58:53,943 - vllm - INFO - WARNING 10-29 19:58:53 [__init__.py:1742] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
2025-10-29 19:58:53,946 - vllm - INFO - [1;36m(APIServer pid=49823)[0;0m INFO 10-29 19:58:53 [api_server.py:1839] vLLM API server version 0.11.0
2025-10-29 19:58:53,950 - vllm - INFO - [1;36m(APIServer pid=49823)[0;0m INFO 10-29 19:58:53 [utils.py:233] non-default args: {'model_tag': 'allenai/olmOCR-2-7B-1025-FP8', 'port': 30024, 'uvicorn_log_level': 'warning', 'model': 'allenai/olmOCR-2-7B-1025-FP8', 'max_model_len': 16384, 'served_model_name': ['olmocr'], 'gpu_memory_utilization': 0.8, 'limit_mm_per_prompt': {'video': 0}}
2025-10-29 19:58:54,468 - vllm - INFO - [1;36m(APIServer pid=49823)[0;0m INFO 10-29 19:58:54 [model.py:547] Resolved architecture: Qwen2_5_VLForConditionalGeneration
2025-10-29 19:58:54,468 - vllm - INFO - [1;36m(APIServer pid=49823)[0;0m INFO 10-29 19:58:54 [model.py:1510] Using max model len 16384
2025-10-29 19:58:54,805 - vllm - INFO - [1;36m(APIServer pid=49823)[0;0m INFO 10-29 19:58:54 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
2025-10-29 19:58:54,872 - __main__ - WARNING - Attempt 10: Please wait for vllm server to become ready...
2025-10-29 19:58:55,909 - __main__ - WARNING - Attempt 11: Please wait for vllm server to become ready...
2025-10-29 19:58:56,949 - __main__ - WARNING - Attempt 12: Please wait for vllm server to become ready...
2025-10-29 19:58:57,989 - __main__ - WARNING - Attempt 13: Please wait for vllm server to become ready...
2025-10-29 19:58:59,047 - __main__ - WARNING - Attempt 14: Please wait for vllm server to become ready...
2025-10-29 19:59:00,020 - vllm - INFO - INFO 10-29 19:59:00 [__init__.py:216] Automatically detected platform cuda.
2025-10-29 19:59:00,100 - __main__ - WARNING - Attempt 15: Please wait for vllm server to become ready...
2025-10-29 19:59:01,141 - __main__ - WARNING - Attempt 16: Please wait for vllm server to become ready...
2025-10-29 19:59:02,179 - __main__ - WARNING - Attempt 17: Please wait for vllm server to become ready...
2025-10-29 19:59:03,224 - __main__ - WARNING - Attempt 18: Please wait for vllm server to become ready...
2025-10-29 19:59:04,266 - __main__ - WARNING - Attempt 19: Please wait for vllm server to become ready...
2025-10-29 19:59:05,143 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:05 [core.py:644] Waiting for init message from front-end.
2025-10-29 19:59:05,155 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:05 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='allenai/olmOCR-2-7B-1025-FP8', speculative_config=None, tokenizer='allenai/olmOCR-2-7B-1025-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=olmocr, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
2025-10-29 19:59:05,308 - __main__ - WARNING - Attempt 20: Please wait for vllm server to become ready...
2025-10-29 19:59:06,346 - __main__ - WARNING - Attempt 21: Please wait for vllm server to become ready...
2025-10-29 19:59:06,815 - vllm - INFO - [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-10-29 19:59:06,834 - vllm - INFO - [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-10-29 19:59:06,841 - vllm - INFO - [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-10-29 19:59:06,847 - vllm - INFO - [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-10-29 19:59:06,852 - vllm - INFO - [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-10-29 19:59:06,859 - vllm - INFO - [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-10-29 19:59:06,860 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:06 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
2025-10-29 19:59:07,140 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m WARNING 10-29 19:59:07 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
2025-10-29 19:59:07,384 - __main__ - WARNING - Attempt 22: Please wait for vllm server to become ready...
2025-10-29 19:59:08,429 - __main__ - WARNING - Attempt 23: Please wait for vllm server to become ready...
2025-10-29 19:59:08,721 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
2025-10-29 19:59:09,468 - __main__ - WARNING - Attempt 24: Please wait for vllm server to become ready...
2025-10-29 19:59:10,506 - __main__ - WARNING - Attempt 25: Please wait for vllm server to become ready...
2025-10-29 19:59:10,749 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:10 [gpu_model_runner.py:2602] Starting to load model allenai/olmOCR-2-7B-1025-FP8...
2025-10-29 19:59:11,185 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:11 [gpu_model_runner.py:2634] Loading model from scratch...
2025-10-29 19:59:11,392 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:11 [cuda.py:366] Using Flash Attention backend on V1 engine.
2025-10-29 19:59:11,570 - __main__ - WARNING - Attempt 26: Please wait for vllm server to become ready...
2025-10-29 19:59:11,673 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
2025-10-29 19:59:11,822 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m 
Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
2025-10-29 19:59:12,609 - __main__ - WARNING - Attempt 27: Please wait for vllm server to become ready...
2025-10-29 19:59:13,664 - __main__ - WARNING - Attempt 28: Please wait for vllm server to become ready...
2025-10-29 19:59:14,702 - __main__ - WARNING - Attempt 29: Please wait for vllm server to become ready...
2025-10-29 19:59:15,746 - __main__ - WARNING - Attempt 30: Please wait for vllm server to become ready...
2025-10-29 19:59:15,881 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m 
Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:04<00:08,  4.06s/it]
2025-10-29 19:59:16,784 - __main__ - WARNING - Attempt 31: Please wait for vllm server to become ready...
2025-10-29 19:59:17,822 - __main__ - WARNING - Attempt 32: Please wait for vllm server to become ready...
2025-10-29 19:59:18,861 - __main__ - WARNING - Attempt 33: Please wait for vllm server to become ready...
2025-10-29 19:59:19,902 - __main__ - WARNING - Attempt 34: Please wait for vllm server to become ready...
2025-10-29 19:59:20,944 - __main__ - WARNING - Attempt 35: Please wait for vllm server to become ready...
2025-10-29 19:59:21,988 - __main__ - WARNING - Attempt 36: Please wait for vllm server to become ready...
2025-10-29 19:59:23,026 - __main__ - WARNING - Attempt 37: Please wait for vllm server to become ready...
2025-10-29 19:59:24,064 - __main__ - WARNING - Attempt 38: Please wait for vllm server to become ready...
2025-10-29 19:59:25,108 - __main__ - WARNING - Attempt 39: Please wait for vllm server to become ready...
2025-10-29 19:59:26,154 - __main__ - WARNING - Attempt 40: Please wait for vllm server to become ready...
2025-10-29 19:59:27,197 - __main__ - WARNING - Attempt 41: Please wait for vllm server to become ready...
2025-10-29 19:59:28,243 - __main__ - WARNING - Attempt 42: Please wait for vllm server to become ready...
2025-10-29 19:59:29,040 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m 
Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:17<00:09,  9.41s/it]
2025-10-29 19:59:29,281 - __main__ - WARNING - Attempt 43: Please wait for vllm server to become ready...
2025-10-29 19:59:30,325 - __main__ - WARNING - Attempt 44: Please wait for vllm server to become ready...
2025-10-29 19:59:31,364 - __main__ - WARNING - Attempt 45: Please wait for vllm server to become ready...
2025-10-29 19:59:31,777 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:19<00:00,  6.36s/it]
2025-10-29 19:59:31,777 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m 
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:19<00:00,  6.65s/it]
2025-10-29 19:59:31,777 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m
2025-10-29 19:59:31,801 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:31 [default_loader.py:267] Loading weights took 19.98 seconds
2025-10-29 19:59:32,401 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:32 [gpu_model_runner.py:2653] Model loading took 9.5222 GiB and 20.641897 seconds
2025-10-29 19:59:32,402 - __main__ - WARNING - Attempt 46: Please wait for vllm server to become ready...
2025-10-29 19:59:32,674 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:32 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
2025-10-29 19:59:33,442 - __main__ - WARNING - Attempt 47: Please wait for vllm server to become ready...
2025-10-29 19:59:34,493 - __main__ - WARNING - Attempt 48: Please wait for vllm server to become ready...
2025-10-29 19:59:35,532 - __main__ - WARNING - Attempt 49: Please wait for vllm server to become ready...
2025-10-29 19:59:36,570 - __main__ - WARNING - Attempt 50: Please wait for vllm server to become ready...
2025-10-29 19:59:37,609 - __main__ - WARNING - Attempt 51: Please wait for vllm server to become ready...
2025-10-29 19:59:38,658 - __main__ - WARNING - Attempt 52: Please wait for vllm server to become ready...
2025-10-29 19:59:39,697 - __main__ - WARNING - Attempt 53: Please wait for vllm server to become ready...
2025-10-29 19:59:40,734 - __main__ - WARNING - Attempt 54: Please wait for vllm server to become ready...
2025-10-29 19:59:41,773 - __main__ - WARNING - Attempt 55: Please wait for vllm server to become ready...
2025-10-29 19:59:42,812 - __main__ - WARNING - Attempt 56: Please wait for vllm server to become ready...
2025-10-29 19:59:43,851 - __main__ - WARNING - Attempt 57: Please wait for vllm server to become ready...
2025-10-29 19:59:44,901 - __main__ - WARNING - Attempt 58: Please wait for vllm server to become ready...
2025-10-29 19:59:45,939 - __main__ - WARNING - Attempt 59: Please wait for vllm server to become ready...
2025-10-29 19:59:46,977 - __main__ - WARNING - Attempt 60: Please wait for vllm server to become ready...
2025-10-29 19:59:48,016 - __main__ - WARNING - Attempt 61: Please wait for vllm server to become ready...
2025-10-29 19:59:49,055 - __main__ - WARNING - Attempt 62: Please wait for vllm server to become ready...
2025-10-29 19:59:50,094 - __main__ - WARNING - Attempt 63: Please wait for vllm server to become ready...
2025-10-29 19:59:51,133 - __main__ - WARNING - Attempt 64: Please wait for vllm server to become ready...
2025-10-29 19:59:52,182 - __main__ - WARNING - Attempt 65: Please wait for vllm server to become ready...
2025-10-29 19:59:52,961 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:52 [backends.py:548] Using cache directory: /home/bryanjowers/.cache/vllm/torch_compile_cache/7bb749dc03/rank_0_0/backbone for vLLM's torch.compile
2025-10-29 19:59:52,963 - vllm - INFO - [1;36m(EngineCore_DP0 pid=49877)[0;0m INFO 10-29 19:59:52 [backends.py:559] Dynamo bytecode transform time: 11.37 s
2025-10-29 19:59:53,255 - __main__ - WARNING - Attempt 66: Please wait for vllm server to become ready...
2025-10-29 19:59:54,293 - __main__ - WARNING - Attempt 67: Please wait for vllm server to become ready...
2025-10-29 19:59:55,332 - __main__ - WARNING - Attempt 68: Please wait for vllm server to become ready...
